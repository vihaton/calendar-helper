{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import numpy as np\n",
    "import arrow\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from source.utils import authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig = False\n",
    "download_all_events = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = authenticate(verbose=True)\n",
    "service = build('calendar', 'v3', credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calendars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clist = service.calendarList().list().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_citem(citem):\n",
    "    print('Summary:\\t' + citem['summary'])\n",
    "    print('id:\\t\\t' + citem['id'])\n",
    "    print('***********')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for citem in clist['items']:\n",
    "    print_citem(citem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary calendar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_default_calendar(clist):\n",
    "    for citem in clist['items']:\n",
    "        if citem.get('primary', False):\n",
    "            print_citem(citem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_default_calendar(clist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_colors = service.colors().get().execute()['event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_col = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfile = data_dir + 'col2meaning.json'\n",
    "with open(keyfile, 'r') as fo:\n",
    "    col2meaning = json.load(fo)\n",
    "col2meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = list(col2meaning.values())\n",
    "if 'default' in types:\n",
    "    types.remove('default')\n",
    "types = np.array(types)\n",
    "list(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2014,1,1)\n",
    "end = datetime(start.year + 1, 1,1)\n",
    "now = arrow.get(datetime.now().isoformat() + '+02:00').datetime\n",
    "\n",
    "end = now\n",
    "\n",
    "start, end, now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_event(e):\n",
    "    print('Summary:\\t' + e.get('summary', 'summary'))\n",
    "    print('ColorId:\\t' + e.get('colorId', 'unknown'))\n",
    "    print('Start:\\t\\t' + str(e.get('start', 'start')))\n",
    "    print('End:\\t\\t' + str(e.get('end', 'end')))\n",
    "    print('Status:\\t\\t' + e.get('status', 'status'))\n",
    "    print('====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_req(min_ts = start.isoformat() + 'Z', max_ts = end.isoformat() + 'Z'):\n",
    "\n",
    "    print('get everything since', min_ts, 'until', max_ts)\n",
    "\n",
    "    req_orig = service.events().list(calendarId='primary',\n",
    "                                          timeMin=min_ts,\n",
    "                                          #timeMax=max_ts,\n",
    "                                          #maxResults=15, \n",
    "                                          singleEvents=True,\n",
    "                                          orderBy='startTime')\n",
    "    return req_orig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_orig = prepare_req()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "col2meaning = { \n",
    "    e.get('colorId'): e.get('summary') for e in events[:-3]\n",
    "}\n",
    "\n",
    "col2meaning\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_event_res(events_result):\n",
    "    print(f'got {len(events_result.get(\"items\", []))} results')\n",
    "    print('next page token:  ' + events_result.get('nextPageToken', 'no next page'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "events_result = req_orig.execute()\n",
    "events = events_result.get('items', [])\n",
    "print_event(events[0])\n",
    "\n",
    "print_event_res(events_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_to_datetime(time_obj):\n",
    "    if type(time_obj) == datetime or type(time_obj) == 'float':\n",
    "        return time_obj\n",
    "    \n",
    "    if 'date' in time_obj:\n",
    "        s = time_obj['date']\n",
    "    elif 'dateTime' in time_obj:\n",
    "        s = time_obj['dateTime']\n",
    "    else:\n",
    "        s = time_obj\n",
    "        \n",
    "    return arrow.get(s).datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_events(verbose=False):\n",
    "    events = []\n",
    "    prev_req = req_orig = prepare_req()\n",
    "    events_result = req_orig.execute()\n",
    "\n",
    "    events.append(events_result.get('items', []))\n",
    "    prev_res = events_result\n",
    "\n",
    "    i = 0\n",
    "    while prev_req is not None:\n",
    "        print(f'round {i}', end='\\r')\n",
    "        prev_req = service.events().list_next(prev_req, prev_res)\n",
    "        if prev_req is None:\n",
    "            break\n",
    "        prev_res = prev_req.execute()\n",
    "        \n",
    "        if verbose: \n",
    "            print_event_res(prev_res)\n",
    "\n",
    "        res_events = prev_res.get('items', [])\n",
    "\n",
    "        events.append(res_events)\n",
    "\n",
    "        last_start = parse_to_datetime(res_events[-1].get('start'))\n",
    "        if last_start > now:\n",
    "            print('last start was in the future, we can stop', last_start)\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    print('Finished fetching all the events')\n",
    "    \n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_event(e):\n",
    "    start = e.get('start')\n",
    "    col = e.get('colorId', '0')\n",
    "    end = e.get('end')\n",
    "    summary = e.get('summary', '')\n",
    "\n",
    "    whole_day = 'date' in start\n",
    "    \n",
    "    start = parse_to_datetime(start)\n",
    "    end = parse_to_datetime(end)\n",
    "\n",
    "    duration_s = (end - start).total_seconds() # <-- total seconds would compute also the full day events\n",
    "    t = col2meaning[col]\n",
    "\n",
    "    return {\n",
    "        'start': start,\n",
    "        'type': t,\n",
    "        'summary': summary,\n",
    "        'duration_s': duration_s,\n",
    "        'whole_day': whole_day,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_datetime_by_week(ts):\n",
    "    year, week = ts.isocalendar()[:2]\n",
    "    return datetime.strptime(f'{year}-{min(week*7, 365)}', \"%Y-%j\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quarter(ts):\n",
    "    mon = ts.month\n",
    "    quartal = mon // 3\n",
    "    quartal += 1 if mon % 3 != 0 else 0 \n",
    "    return quartal\n",
    "\n",
    "def get_invoiced_quarter(ts):\n",
    "    if type(ts) is float:\n",
    "        return 666 # ts was not defined\n",
    "    \n",
    "    assert type(ts) is str, 'expected timestamp to be a string'\n",
    "    mon = int(ts[5:7])\n",
    "    quartal = mon // 3\n",
    "    quartal += 1 if mon % 3 != 0 else 0 \n",
    "    return quartal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and save the events "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_events(df, time_str):\n",
    "    df.to_csv(data_dir + f'calendar_events_until_{time_str}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if download_all_events:\n",
    "    events = download_events()\n",
    "    # includes all recurring future events as well\n",
    "    events = np.concatenate(events) if type(events) is list else events\n",
    "    print('events.shape', events.shape)\n",
    "    \n",
    "    print('available fields')\n",
    "    print(events[0].keys())\n",
    "    print()\n",
    "    \n",
    "    print_event(events[0])\n",
    "    print_event(events[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_all_events:\n",
    "    print('parsing events...')\n",
    "    events_parsed = [ parse_event(e) for e in events ]\n",
    "    \n",
    "    events_all = pd.DataFrame(data=events_parsed)\n",
    "\n",
    "    yesterday = now - timedelta(days=1)\n",
    "    print(yesterday)\n",
    "    \n",
    "    hist_df = events_all[events_all.start < yesterday]\n",
    "\n",
    "    save_events(hist_df, yesterday.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
    "    print('hist df saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the events from disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_dir(directory, return_dirs=False, verbose=False):\n",
    "    if not os.path.exists(directory):\n",
    "        raise Exception(f'{directory} does not exist!')\n",
    "    for (path, dirs, files) in os.walk(directory):\n",
    "        if verbose:\n",
    "            print('path: ', path)\n",
    "            print('dirs', dirs)\n",
    "            print('files')\n",
    "            for i, file in enumerate(files):\n",
    "                print('\\t', i, file)\n",
    "        break\n",
    "    return files if not return_dirs else (files, dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files_in_dir(data_dir)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = -1\n",
    "assert files[ind].split('.')[-1] in 'csv', f'file was not a csv file! it was {files[ind]}'\n",
    "\n",
    "events_file = data_dir + files[ind]\n",
    "print(f'load events from file {events_file}')\n",
    "\n",
    "with open(events_file, 'rb') as fo:\n",
    "    events_all = pd.read_csv(events_file)\n",
    "    \n",
    "events_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Durations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_durations(df):\n",
    "    df['duration_min'] = df['duration_s'] / 60\n",
    "    df['duration_h'] = df['duration_min'] / 60\n",
    "    df['duration_d'] = df['duration_h'] / 24\n",
    "    df['is_full_day'] = (df['duration_d'] >= 1) & (df['duration_h'] % 24 - 0 < 1e-6)\n",
    "    df['whole_day'] = df['whole_day'] | df['is_full_day'] # this adds couple outlier events\n",
    "    df['is_over_24h'] = df['duration_h'] > 24\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_all = process_durations(events_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_all[~events_all.is_full_day & events_all.whole_day] # should be empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_all[events_all.duration_d >= 1].sort_values('duration_d', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dates(df):\n",
    "    df['start'] = df.start.apply(lambda ts: parse_to_datetime(ts))\n",
    "    df['year'] = df.start.apply(lambda ts: ts.year) #datetime.strptime(f'{ts.year}-1-1', '%Y-%m-%d'))\n",
    "    df['year_mon'] = df.start.apply(lambda ts: datetime.strptime(f'{ts.year}-{ts.month}-1', '%Y-%m-%d'))\n",
    "    df['mon'] = df.start.apply(lambda ts: datetime.strptime(f'2000-{ts.month}-1', '%Y-%m-%d'))\n",
    "    df['year_week'] = df.start.apply(group_datetime_by_week)\n",
    "    df['quarter'] = df.start.apply(get_quarter)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_all = process_dates(events_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_year = events_all.year.max()\n",
    "print('max year', max_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = events_all[(events_all.whole_day == False) & (events_all.is_over_24h == False)]\n",
    "events.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_missing_types(data, time_col='year_week', unique_times=None):\n",
    "    \n",
    "    unique_times = unique_times if unique_times is not None else data[time_col].unique()\n",
    "    \n",
    "    to_append = []\n",
    "    for week in unique_times:\n",
    "        entries = data[data[time_col] == week]\n",
    "\n",
    "        type_present = [False]*len(types)\n",
    "        for ind, row in entries.iterrows():\n",
    "            i = np.argmax(row.type == types)\n",
    "            type_present[i] = True\n",
    "\n",
    "        for pres, t in zip(type_present, types):\n",
    "            if not pres:\n",
    "                to_append.append({'type': t, time_col: week, 'duration_h': 0})\n",
    "\n",
    "\n",
    "    data = pd.concat([data, pd.DataFrame(to_append)], ignore_index=True)\n",
    "    data = data.sort_values([time_col, 'type'])\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_year = events.groupby(['type', 'year']).agg({'duration_h': np.sum}).reset_index()\n",
    "events_year = fill_in_missing_types(events_year, 'year')\n",
    "events_year = events_year.sort_values(['year', 'type'])\n",
    "events_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_week = events.groupby(['type', 'year_week']).agg({'duration_h': np.sum}).reset_index()\n",
    "events_week = fill_in_missing_types(events_week, 'year_week', events_all.year_week.unique())\n",
    "events_week['year'] = events_week.year_week.apply(lambda ts: ts.year)\n",
    "events_week = events_week.sort_values(['year_week', 'type'])\n",
    "events_week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_colors['0'] = dict(background='blue')\n",
    "event_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = {\n",
    "    col2meaning[c]: event_colors[c]['background'] for c in event_colors\n",
    "}\n",
    "palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_values_on_bars(axs):\n",
    "    def _show_on_single_plot(ax):        \n",
    "        for p in ax.patches:\n",
    "            _x = p.get_x() + p.get_width() / 2\n",
    "            _y = p.get_y() + p.get_height()\n",
    "            value = str(int(round(p.get_height(),0)))\n",
    "            ax.text(_x, _y, value, ha=\"center\", fontsize=8) \n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _show_on_single_plot(ax)\n",
    "    else:\n",
    "        _show_on_single_plot(axs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(12, 8), dpi=120, facecolor=\"w\")\n",
    "\n",
    "data = events.groupby(\"type\").agg({\"duration_d\": np.sum}).reset_index()\n",
    "\n",
    "sns.barplot(\n",
    "    data=data.sort_values(\"type\"), x=\"type\", y=\"duration_d\", hue=\"type\", ax=axes, palette=palette\n",
    ")\n",
    "show_values_on_bars(axes)\n",
    "plt.title(f\"Use of time per event type from 2015-{max_year}\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,1,figsize=(12,8), dpi=120, facecolor='w')\n",
    "\n",
    "sns.barplot(data=events.sort_values('type'), x='type', y='duration_h', ax=axes, palette=palette)\n",
    "plt.title('distribution of event duration')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event types over time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_agg = 'duration_h'\n",
    "data = events.groupby(['type', 'year']).agg({time_agg: np.sum}).reset_index()\n",
    "data = data.sort_values('year')\n",
    "\n",
    "n_years = data.year.nunique()\n",
    "\n",
    "top = 1.05 * data[time_agg].max()\n",
    "\n",
    "fig, axes = plt.subplots(n_years,1,figsize=(12,6*n_years), dpi=120, facecolor='w')\n",
    "\n",
    "for i, year in enumerate(data.year.unique()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    query = f'year == {year}'\n",
    "    sns.barplot(data=data.query(query).sort_values('type'), x='type', y=time_agg, ax=ax, palette=palette, hue='type')\n",
    "    ax.set_title(f'{year}')\n",
    "    \n",
    "    show_values_on_bars(ax)\n",
    "    ax.set_ylim(top=top)\n",
    "    ax.xaxis.set_tick_params(rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By week "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_agg = 'duration_h'\n",
    "data = events_week\n",
    "n_years = data.year.nunique()\n",
    "\n",
    "data = fill_in_missing_types(data, 'year_week')\n",
    "\n",
    "top = 1.05 * data[time_agg].max()\n",
    "\n",
    "fig, axes = plt.subplots(n_years,1,figsize=(12,6*n_years), dpi=120, facecolor='w')\n",
    "\n",
    "for i, year in enumerate(events.year.unique()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    query = f'year == {year}'\n",
    "    sns.lineplot(data=data.query(query).sort_values('type'), x='year_week', y=time_agg, hue='type', ax=ax, palette=palette)\n",
    "    ax.set_title(f'{year}')\n",
    "    \n",
    "    ax.set_ylim(top=top)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(types):\n",
    "    print(i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_types = [types[3], types[8], types[9]]\n",
    "work_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_agg = 'duration_h'\n",
    "\n",
    "any_work_hours = np.array([data[data.type == t][time_agg] for t in work_types]).sum(axis=0)\n",
    "any_work_hours[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = events_week\n",
    "\n",
    "fig, axes = plt.subplots(1,1,figsize=(18,8), dpi=120, facecolor='w')\n",
    "\n",
    "ax = axes\n",
    "\n",
    "\n",
    "query = f'type == \"{types[0]}\"'\n",
    "sns.lineplot(x=data.year_week.unique(), y=data.query(query)[time_agg], ax=ax, label=types[0])\n",
    "\n",
    "sns.lineplot(x=data.year_week.unique(), y=any_work_hours, ax=ax, label='efficient working hours')\n",
    "\n",
    "ax.axhline(37.5, ls='--', color='k', zorder=0, label='full work week')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Efficient working hours per week')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = pd.DataFrame(data={'year_week': data.year_week.unique(), 'working_h': any_work_hours})\n",
    "working_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df['overtime'] = working_df.working_h.apply(lambda x: x >= 37.5)\n",
    "working_df['clear overtime'] = working_df.working_h.apply(lambda x: x >= 40)\n",
    "working_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Busiest weeks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df_sorted = working_df.sort_values(['working_h', 'year_week'], ascending=[False, True]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data to create a matrix for the heatmap\n",
    "heatmap_data_month = working_df.pivot_table(\n",
    "    index=working_df[\"year_week\"].dt.year,\n",
    "    columns=working_df[\"year_week\"].dt.month,\n",
    "    values=\"working_h\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0,\n",
    ").rename_axis(\"Month\", axis=1).rename_axis(\"Year\", axis=0)\n",
    "\n",
    "# Drop rows where the sum of the row is zero\n",
    "heatmap_data_month.drop(\n",
    "    heatmap_data_month.sum(axis=1)[heatmap_data_month.sum(axis=1) == 0].index, inplace=True\n",
    ")\n",
    "\n",
    "heatmap_data_month = heatmap_data_month / 7.5\n",
    "heatmap_data_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data_month, cmap=\"coolwarm\", cbar_kws={'label': 'Working Days (PWD); 21 days is a full working month'}, annot=True, fmt=\".0f\", center=21)\n",
    "plt.title(\"Heatmap of Work Time (PWD) by Month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Year\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data to create a matrix for the heatmap\n",
    "heatmap_data_week = working_df.pivot_table(\n",
    "    index=working_df[\"year_week\"].dt.year,\n",
    "    columns=working_df[\"year_week\"].dt.isocalendar().week,\n",
    "    values=\"working_h\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0,\n",
    ").rename_axis(\"Week\", axis=1).rename_axis(\"Year\", axis=0)\n",
    "\n",
    "# remove w53\n",
    "heatmap_data_week.loc[:, 52] += heatmap_data_week.loc[:, 53]\n",
    "heatmap_data_week.drop(53, inplace=True, axis=1)\n",
    "\n",
    "# remove years before 2018\n",
    "heatmap_data_week = heatmap_data_week.loc[2018:]\n",
    "\n",
    "# Drop rows where the sum of the row is zero\n",
    "heatmap_data_week.drop(\n",
    "    heatmap_data_week.sum(axis=1)[heatmap_data_week.sum(axis=1) == 0].index, inplace=True\n",
    ")\n",
    "\n",
    "heatmap_data_week = heatmap_data_week / 7.5\n",
    "heatmap_data_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data to create a matrix for the heatmap\n",
    "heatmap_data_overtime = working_df.pivot_table(\n",
    "    index=working_df[\"year_week\"].dt.year,\n",
    "    columns=working_df[\"year_week\"].dt.isocalendar().week,\n",
    "    values=\"overtime\",\n",
    "    # aggfunc=\"sum\",\n",
    "    fill_value=0,\n",
    ").rename_axis(\"Week\", axis=1).rename_axis(\"Year\", axis=0)\n",
    "\n",
    "heatmap_data_overtime.drop(53, inplace=True, axis=1)\n",
    "\n",
    "# # remove years before 2018\n",
    "heatmap_data_overtime = heatmap_data_overtime.loc[2018:]\n",
    "\n",
    "heatmap_data_overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the heatmap\n",
    "plt.figure(figsize=(15, 8))\n",
    "how_much_overtime_week = (heatmap_data_week-5).where(heatmap_data_week > 5, other=0)\n",
    "sns.heatmap(heatmap_data_overtime, cmap=\"coolwarm\", cbar_kws={'label': 'Working Days (PWD); 5 days is a full work week'}, annot=how_much_overtime_week, fmt=\".0f\")\n",
    "plt.title(\"Heatmap of Weeks with Overtime (PWD) by Week\")\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Year\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the heatmap\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(how_much_overtime_week, cmap=\"coolwarm\", cbar_kws={'label': 'Working Days (PWD); 5 days is a full work week'}, annot=True, fmt=\".0f\", center=1)\n",
    "plt.title(\"Heatmap of Weeks with Overtime (PWD) by Week\")\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Year\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overtime 2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_20 = working_df[np.bitwise_and(working_df.year_week > datetime(2020, 1, 1), working_df.year_week < datetime(2021,1,1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_20.groupby('overtime').nunique()['year_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_20.groupby('clear overtime').nunique()['year_week']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calendar-helper (3.10.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
